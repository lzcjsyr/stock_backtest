"""
å¼‚æ­¥æ‰¹é‡ä¸‹è½½å™¨ - é«˜é€Ÿä¸‹è½½å…¨éƒ¨Aè‚¡æ•°æ®

è¿™ä¸ªè„šæœ¬ä½¿ç”¨å¼‚æ­¥ç¼–ç¨‹æ¥å¤§å¹…æå‡ä¸‹è½½é€Ÿåº¦
æ”¯æŒå¹¶å‘ä¸‹è½½ã€æ™ºèƒ½é‡è¯•ã€æ–­ç‚¹ç»­ä¼ 

ä½¿ç”¨æ–¹æ³•ï¼š
ç›´æ¥è¿è¡Œè„šæœ¬ï¼ŒæŒ‰æç¤ºè¾“å…¥å‚æ•°å³å¯
python async_batch_downloader.py

ä½œè€…ï¼šClaude Code
"""

import asyncio
import akshare as ak
import pandas as pd
import argparse
from datetime import datetime, timedelta
from stock_database import StockDatabase
import logging
import threading

# ==================== ç”¨æˆ·é…ç½®å‚æ•° ====================
# ä»¥ä¸‹å‚æ•°å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´

# å¼‚æ­¥ä¸‹è½½é…ç½®
DEFAULT_MAX_CONCURRENT = 10      # é»˜è®¤æœ€å¤§å¹¶å‘ä¸‹è½½æ•°
DEFAULT_DELAY_SECONDS = 0.5      # è¯·æ±‚é—´å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
DEFAULT_BATCH_SIZE = 200         # æ¯æ‰¹å¤„ç†çš„è‚¡ç¥¨æ•°é‡
DEFAULT_DAYS = 30                # é»˜è®¤ä¸‹è½½æœ€è¿‘å¤šå°‘å¤©çš„æ•°æ®

# ç½‘ç»œé…ç½®  
MAX_RETRIES = 3                  # ç½‘ç»œå¤±è´¥æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°

# å¸¸ç”¨æ—¶é—´é€‰é¡¹
COMMON_TIME_OPTIONS = {
    "1": ("æœ€è¿‘7å¤©", 7),
    "2": ("æœ€è¿‘30å¤©", 30), 
    "3": ("æœ€è¿‘90å¤©", 90),
    "4": ("æœ€è¿‘180å¤©", 180),
    "5": ("æœ€è¿‘ä¸€å¹´", 365),
    "6": ("è‡ªå®šä¹‰æ—¥æœŸèŒƒå›´", None),
    "7": ("ä»æŒ‡å®šæ—¥æœŸåˆ°ä»Šå¤©", None)
}

# å¹¶å‘æ•°é€‰é¡¹
CONCURRENT_OPTIONS = {
    "1": ("ä¿å®ˆæ¨¡å¼", 5),
    "2": ("æ ‡å‡†æ¨¡å¼", 10),
    "3": ("æ¿€è¿›æ¨¡å¼", 20),
    "4": ("æé€Ÿæ¨¡å¼", 30),
    "5": ("è‡ªå®šä¹‰", None)
}

# ======================================================

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

class AsyncStockDownloader:
    """
    å¼‚æ­¥è‚¡ç¥¨ä¸‹è½½å™¨ - é«˜é€Ÿå¹¶å‘ä¸‹è½½
    """
    
    def __init__(self, max_concurrent=10, delay_seconds=0.5, max_retries=3):
        """
        åˆå§‹åŒ–å¼‚æ­¥ä¸‹è½½å™¨
        
        Parameters:
        -----------
        max_concurrent : int
            æœ€å¤§å¹¶å‘ä¸‹è½½æ•°
        delay_seconds : float
            è¯·æ±‚é—´å»¶è¿Ÿæ—¶é—´
        max_retries : int
            æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.db = StockDatabase()
        self.max_concurrent = max_concurrent
        self.delay_seconds = delay_seconds
        self.max_retries = max_retries
        self.session = None
        
        # ç»Ÿè®¡å˜é‡
        self.success_count = 0
        self.failed_count = 0
        self.lock = threading.Lock()
        
        logger.info(f"ğŸš€ å¼‚æ­¥ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   - æœ€å¤§å¹¶å‘: {max_concurrent}")
        logger.info(f"   - è¯·æ±‚å»¶è¿Ÿ: {delay_seconds}ç§’")
        logger.info(f"   - æœ€å¤§é‡è¯•: {max_retries}æ¬¡")
    
    def _get_stock_symbol(self, stock_code):
        """è·å–è‚¡ç¥¨ä»£ç æ ¼å¼"""
        # ä¸Šæµ·è‚¡ç¥¨ï¼šä¸»æ¿600/601/603ã€ç§‘åˆ›æ¿688
        if stock_code.startswith(('60', '688')):
            return f"sh{stock_code}"
        # æ·±åœ³è‚¡ç¥¨ï¼šä¸»æ¿000/001ã€åˆ›ä¸šæ¿300
        elif stock_code.startswith(('000', '001', '002', '003', '300', '301')):
            return f"sz{stock_code}"
        # åŒ—äº¤æ‰€ï¼š8å¼€å¤´
        elif stock_code.startswith('8'):
            return f"bj{stock_code}"
        else:
            return f"sz{stock_code}"
    
    def _convert_data_format(self, raw_data):
        """è½¬æ¢æ•°æ®æ ¼å¼"""
        if raw_data.empty:
            return raw_data
        
        try:
            data = raw_data.copy()
            
            # å­—æ®µæ˜ å°„
            column_mapping = {
                'date': 'æ—¥æœŸ',
                'open': 'å¼€ç›˜',
                'high': 'æœ€é«˜',
                'low': 'æœ€ä½',
                'close': 'æ”¶ç›˜',
                'volume': 'æˆäº¤é‡',
                'amount': 'æˆäº¤é¢'
            }
            
            data = data.rename(columns=column_mapping)
            
            # æ·»åŠ é»˜è®¤å­—æ®µ
            default_fields = {
                'æ¶¨è·Œå¹…': 0.0,
                'æ¶¨è·Œé¢': 0.0, 
                'æ¢æ‰‹ç‡': 0.0,
                'æŒ¯å¹…': 0.0
            }
            
            for field, default_value in default_fields.items():
                if field not in data.columns:
                    data[field] = default_value
            
            data['æ—¥æœŸ'] = pd.to_datetime(data['æ—¥æœŸ'])
            return data
            
        except Exception as e:
            logger.error(f"æ•°æ®æ ¼å¼è½¬æ¢å¤±è´¥: {e}")
            return raw_data
    
    async def download_single_stock_async(self, stock_code, start_date, end_date, force_update=False):
        """
        å¼‚æ­¥ä¸‹è½½å•åªè‚¡ç¥¨æ•°æ®
        """
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®
            if not force_update and self.db.check_kline_data_exists(stock_code, start_date, end_date):
                with self.lock:
                    self.success_count += 1
                return True
            
            # å‡†å¤‡å‚æ•°
            symbol = self._get_stock_symbol(stock_code)
            start_str = start_date.replace('-', '')
            end_str = end_date.replace('-', '')
            
            # é‡è¯•æœºåˆ¶
            for attempt in range(self.max_retries):
                try:
                    # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œakshareè°ƒç”¨ï¼ˆå› ä¸ºakshareä¸æ˜¯å¼‚æ­¥çš„ï¼‰
                    loop = asyncio.get_event_loop()
                    raw_data = await loop.run_in_executor(
                        None,
                        lambda: ak.stock_zh_a_daily(
                            symbol=symbol,
                            start_date=start_str,
                            end_date=end_str,
                            adjust="qfq"
                        )
                    )
                    
                    if not raw_data.empty:
                        # è½¬æ¢æ ¼å¼å¹¶ä¿å­˜
                        standard_data = self._convert_data_format(raw_data)
                        if self.db.save_daily_kline_data(standard_data, stock_code):
                            with self.lock:
                                self.success_count += 1
                            return True
                        else:
                            raise Exception("ä¿å­˜æ•°æ®åº“å¤±è´¥")
                    else:
                        raise Exception("è¿”å›ç©ºæ•°æ®")
                        
                except Exception as e:
                    if attempt < self.max_retries - 1:
                        await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    else:
                        logger.warning(f"âŒ {stock_code} ä¸‹è½½å¤±è´¥: {e}")
            
            with self.lock:
                self.failed_count += 1
            return False
            
        except Exception as e:
            logger.error(f"âŒ {stock_code} ä¸‹è½½è¿‡ç¨‹å‡ºé”™: {e}")
            with self.lock:
                self.failed_count += 1
            return False
    
    async def download_batch_async(self, stock_codes, start_date, end_date, force_update=False):
        """
        å¼‚æ­¥æ‰¹é‡ä¸‹è½½è‚¡ç¥¨æ•°æ®
        """
        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async def download_with_semaphore(stock_code):
            async with semaphore:
                result = await self.download_single_stock_async(
                    stock_code, start_date, end_date, force_update
                )
                # æ·»åŠ å»¶è¿Ÿ
                if self.delay_seconds > 0:
                    await asyncio.sleep(self.delay_seconds)
                return result
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [download_with_semaphore(code) for code in stock_codes]
        
        # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return results

class AsyncBatchDownloader:
    """
    å¼‚æ­¥æ‰¹é‡ä¸‹è½½å™¨ä¸»ç±»
    """
    
    def __init__(self, max_concurrent=10, delay_seconds=0.5, batch_size=200):
        """
        åˆå§‹åŒ–
        
        Parameters:
        -----------
        max_concurrent : int
            æœ€å¤§å¹¶å‘æ•°
        delay_seconds : float
            è¯·æ±‚å»¶è¿Ÿ
        batch_size : int
            æ¯æ‰¹å¤„ç†æ•°é‡
        """
        self.downloader = AsyncStockDownloader(max_concurrent, delay_seconds)
        self.batch_size = batch_size
        self.db = StockDatabase()
        
        logger.info(f"ğŸš€ å¼‚æ­¥æ‰¹é‡ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   - æ¯æ‰¹è‚¡ç¥¨æ•°: {batch_size}")
    
    def get_all_stock_codes(self):
        """è·å–æ‰€æœ‰æ´»è·ƒè‚¡ç¥¨ä»£ç """
        conn = self.db.get_connection()
        if not conn:
            logger.error("æ•°æ®åº“è¿æ¥å¤±è´¥")
            return []
        
        try:
            cursor = conn.cursor()
            cursor.execute("SELECT stock_code FROM stock_basic_info WHERE is_active = 1")
            results = cursor.fetchall()
            stock_codes = [row[0] for row in results]
            logger.info(f"ğŸ“‹ ä»æ•°æ®åº“è·å–åˆ° {len(stock_codes)} åªæ´»è·ƒè‚¡ç¥¨")
            return stock_codes
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
        finally:
            conn.close()
    
    async def download_all_stocks_async(self, days=None, start_date=None, end_date=None, force_update=False):
        """
        å¼‚æ­¥ä¸‹è½½æ‰€æœ‰è‚¡ç¥¨æ•°æ®
        """
        # è·å–æ‰€æœ‰è‚¡ç¥¨
        all_stocks = self.get_all_stock_codes()
        total_stocks = len(all_stocks)
        
        if total_stocks == 0:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°æ´»è·ƒè‚¡ç¥¨")
            return
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        if days:
            end_date = datetime.now().strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        
        logger.info(f"ğŸ“‹ å‡†å¤‡å¼‚æ­¥ä¸‹è½½ {total_stocks} åªè‚¡ç¥¨çš„æ•°æ®")
        logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
        
        start_time = datetime.now()
        logger.info(f"ğŸ• å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, total_stocks, self.batch_size):
            batch_stocks = all_stocks[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (total_stocks + self.batch_size - 1) // self.batch_size
            
            logger.info(f"\nğŸ“¦ å¤„ç†ç¬¬ {batch_num}/{total_batches} æ‰¹ ({len(batch_stocks)} åªè‚¡ç¥¨)")
            
            # å¼‚æ­¥ä¸‹è½½å½“å‰æ‰¹æ¬¡
            batch_start = datetime.now()
            await self.downloader.download_batch_async(
                batch_stocks, start_date, end_date, force_update
            )
            batch_end = datetime.now()
            
            # æ˜¾ç¤ºæ‰¹æ¬¡ç»“æœ
            batch_time = batch_end - batch_start
            speed = len(batch_stocks) / batch_time.total_seconds()
            
            logger.info(f"   âš¡ æ‰¹æ¬¡è€—æ—¶: {batch_time.total_seconds():.1f}ç§’")
            logger.info(f"   ğŸï¸  æ‰¹æ¬¡é€Ÿåº¦: {speed:.1f} è‚¡ç¥¨/ç§’")
            
            # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
            processed = min(i + self.batch_size, total_stocks)
            progress = (processed / total_stocks) * 100
            logger.info(f"ğŸ“Š æ€»è¿›åº¦: {processed}/{total_stocks} ({progress:.1f}%)")
            logger.info(f"   æˆåŠŸ: {self.downloader.success_count}, å¤±è´¥: {self.downloader.failed_count}")
            
            # ä¼°ç®—å‰©ä½™æ—¶é—´
            if processed > 0:
                elapsed = datetime.now() - start_time
                avg_time_per_stock = elapsed.total_seconds() / processed
                remaining_stocks = total_stocks - processed
                estimated_remaining = remaining_stocks * avg_time_per_stock
                
                if estimated_remaining > 60:
                    remaining_minutes = int(estimated_remaining / 60)
                    logger.info(f"â±ï¸  é¢„è®¡å‰©ä½™: {remaining_minutes} åˆ†é’Ÿ")
        
        # å®Œæˆç»Ÿè®¡
        end_time = datetime.now()
        total_time = end_time - start_time
        
        logger.info(f"\n" + "="*60)
        logger.info(f"ğŸ¯ å¼‚æ­¥ä¸‹è½½å®Œæˆ!")
        logger.info(f"   âœ… æˆåŠŸ: {self.downloader.success_count}")
        logger.info(f"   âŒ å¤±è´¥: {self.downloader.failed_count}")
        logger.info(f"   ğŸ“Š æ€»è®¡: {total_stocks}")
        logger.info(f"   ğŸ• æ€»è€—æ—¶: {total_time}")
        logger.info(f"   âš¡ å¹³å‡é€Ÿåº¦: {total_stocks/total_time.total_seconds():.2f} è‚¡ç¥¨/ç§’")

def get_user_input():
    """
    äº¤äº’å¼è·å–ç”¨æˆ·è¾“å…¥å‚æ•°
    """
    print("\n" + "="*60)
    print("âš¡ å¼‚æ­¥æ‰¹é‡ä¸‹è½½å…¨éƒ¨Aè‚¡æ•°æ®")
    print("="*60)
    
    # æ˜¾ç¤ºæ—¶é—´é€‰é¡¹
    print("\nğŸ“… è¯·é€‰æ‹©ä¸‹è½½æ—¶é—´èŒƒå›´ï¼š")
    for key, (desc, days) in COMMON_TIME_OPTIONS.items():
        print(f"   {key}. {desc}")
    
    # è·å–æ—¶é—´é€‰æ‹©
    while True:
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹æ•°å­— (1-7): ").strip()
        if choice in COMMON_TIME_OPTIONS:
            break
        print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    desc, days = COMMON_TIME_OPTIONS[choice]
    print(f"âœ… å·²é€‰æ‹©: {desc}")
    
    # æ ¹æ®é€‰æ‹©è·å–å…·ä½“å‚æ•°
    start_date = None
    end_date = None
    days_param = None
    
    if choice == "6":  # è‡ªå®šä¹‰æ—¥æœŸèŒƒå›´
        while True:
            try:
                start_date = input("è¯·è¾“å…¥å¼€å§‹æ—¥æœŸ (æ ¼å¼: 2025-08-01): ").strip()
                datetime.strptime(start_date, '%Y-%m-%d')
                break
            except ValueError:
                print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        
        while True:
            try:
                end_date = input("è¯·è¾“å…¥ç»“æŸæ—¥æœŸ (æ ¼å¼: 2025-08-27): ").strip()
                datetime.strptime(end_date, '%Y-%m-%d')
                break
            except ValueError:
                print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
    
    elif choice == "7":  # ä»æŒ‡å®šæ—¥æœŸåˆ°ä»Šå¤©
        while True:
            try:
                start_date = input("è¯·è¾“å…¥å¼€å§‹æ—¥æœŸ (æ ¼å¼: 2025-08-01): ").strip()
                datetime.strptime(start_date, '%Y-%m-%d')
                end_date = datetime.now().strftime('%Y-%m-%d')
                break
            except ValueError:
                print("âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
    
    else:  # æœ€è¿‘Nå¤©
        days_param = days
    
    # è·å–å¹¶å‘é…ç½®
    print(f"\nğŸš€ è¯·é€‰æ‹©å¹¶å‘æ¨¡å¼ï¼š")
    for key, (desc, concurrent) in CONCURRENT_OPTIONS.items():
        print(f"   {key}. {desc}" + (f" ({concurrent}å¹¶å‘)" if concurrent else ""))
    
    while True:
        concurrent_choice = input("\nè¯·è¾“å…¥é€‰é¡¹æ•°å­— (1-5): ").strip()
        if concurrent_choice in CONCURRENT_OPTIONS:
            break
        print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    desc, concurrent = CONCURRENT_OPTIONS[concurrent_choice]
    print(f"âœ… å·²é€‰æ‹©: {desc}")
    
    if concurrent_choice == "5":  # è‡ªå®šä¹‰
        while True:
            try:
                concurrent = int(input("è¯·è¾“å…¥å¹¶å‘æ•° (å»ºè®®5-30): "))
                if 1 <= concurrent <= 50:
                    break
                print("âŒ å¹¶å‘æ•°åº”åœ¨1-50ä¹‹é—´")
            except ValueError:
                print("âŒ è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
    
    # è·å–å…¶ä»–å‚æ•°
    print(f"\nâš™ï¸  é«˜çº§è®¾ç½® (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤å€¼):")
    
    # å»¶è¿Ÿæ—¶é—´
    delay_input = input(f"è¯·æ±‚å»¶è¿Ÿæ—¶é—´ (é»˜è®¤{DEFAULT_DELAY_SECONDS}ç§’): ").strip()
    delay = float(delay_input) if delay_input else DEFAULT_DELAY_SECONDS
    
    # æ‰¹æ¬¡å¤§å°
    batch_input = input(f"æ¯æ‰¹å¤„ç†è‚¡ç¥¨æ•° (é»˜è®¤{DEFAULT_BATCH_SIZE}): ").strip()
    batch_size = int(batch_input) if batch_input else DEFAULT_BATCH_SIZE
    
    # æ˜¯å¦å¼ºåˆ¶æ›´æ–°
    force_input = input("æ˜¯å¦å¼ºåˆ¶æ›´æ–°å·²æœ‰æ•°æ®? (y/N): ").strip().lower()
    force_update = force_input in ['y', 'yes']
    
    return {
        'days': days_param,
        'start_date': start_date,
        'end_date': end_date,
        'concurrent': concurrent,
        'delay': delay,
        'batch_size': batch_size,
        'force_update': force_update
    }

def main():
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
    if len(__import__('sys').argv) > 1:
        # ä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼
        parser = argparse.ArgumentParser(description="å¼‚æ­¥æ‰¹é‡ä¸‹è½½å…¨éƒ¨Aè‚¡æ•°æ®")
        
        time_group = parser.add_mutually_exclusive_group(required=True)
        time_group.add_argument("--days", type=int, help="æœ€è¿‘å¤©æ•°ï¼Œå¦‚ 30")
        time_group.add_argument("--date-range", nargs=2, metavar=("START", "END"), 
                               help="æ—¥æœŸèŒƒå›´ï¼Œå¦‚ 2025-08-01 2025-08-27")
        time_group.add_argument("--from-date", metavar="START_DATE",
                               help="ä»æŒ‡å®šæ—¥æœŸåˆ°ä»Šå¤©ï¼Œå¦‚ 2025-08-01")
        
        parser.add_argument("--concurrent", type=int, default=DEFAULT_MAX_CONCURRENT, help="æœ€å¤§å¹¶å‘æ•°")
        parser.add_argument("--delay", type=float, default=DEFAULT_DELAY_SECONDS, help="è¯·æ±‚å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰")
        parser.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_SIZE, help="æ¯æ‰¹å¤„ç†è‚¡ç¥¨æ•°")
        parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶æ›´æ–°ï¼Œå¿½ç•¥å·²æœ‰æ•°æ®")
        
        args = parser.parse_args()
        
        params = {
            'concurrent': args.concurrent,
            'delay': args.delay,
            'batch_size': args.batch_size,
            'force_update': args.force
        }
        
        if args.days:
            params['days'] = args.days
        elif args.from_date:
            params['start_date'] = args.from_date
            params['end_date'] = datetime.now().strftime('%Y-%m-%d')
        else:
            params['start_date'], params['end_date'] = args.date_range
    
    else:
        # ä½¿ç”¨äº¤äº’æ¨¡å¼
        params = get_user_input()
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print(f"\nâš¡ å¼‚æ­¥ä¸‹è½½é…ç½®:")
    if params.get('days'):
        print(f"   ğŸ“… æ—¶é—´èŒƒå›´: æœ€è¿‘ {params['days']} å¤©")
    else:
        print(f"   ğŸ“… æ—¶é—´èŒƒå›´: {params['start_date']} åˆ° {params['end_date']}")
    print(f"   ğŸš€ å¹¶å‘æ•°é‡: {params['concurrent']}")
    print(f"   â±ï¸  è¯·æ±‚å»¶è¿Ÿ: {params['delay']} ç§’")
    print(f"   ğŸ“¦ æ‰¹æ¬¡å¤§å°: {params['batch_size']}")
    print(f"   ğŸ”„ å¼ºåˆ¶æ›´æ–°: {'æ˜¯' if params['force_update'] else 'å¦'}")
    
    # ç¡®è®¤å¼€å§‹
    if len(__import__('sys').argv) == 1:  # äº¤äº’æ¨¡å¼æ‰éœ€è¦ç¡®è®¤
        confirm = input(f"\nç¡®è®¤å¼€å§‹å¼‚æ­¥ä¸‹è½½? (Y/n): ").strip().lower()
        if confirm in ['n', 'no']:
            print("âŒ å·²å–æ¶ˆä¸‹è½½")
            return
    
    # åˆ›å»ºå¼‚æ­¥æ‰¹é‡ä¸‹è½½å™¨
    async_downloader = AsyncBatchDownloader(
        max_concurrent=params['concurrent'],
        delay_seconds=params['delay'],
        batch_size=params['batch_size']
    )
    
    # è¿è¡Œå¼‚æ­¥ä¸‹è½½
    async def run_download():
        if params.get('days'):
            await async_downloader.download_all_stocks_async(
                days=params['days'],
                force_update=params['force_update']
            )
        else:
            await async_downloader.download_all_stocks_async(
                start_date=params['start_date'],
                end_date=params['end_date'],
                force_update=params['force_update']
            )
    
    # è¿è¡Œå¼‚æ­¥ç¨‹åº
    asyncio.run(run_download())

if __name__ == "__main__":
    main()